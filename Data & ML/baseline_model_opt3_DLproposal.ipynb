{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a94dcd9",
   "metadata": {},
   "source": [
    "# General overview on Recommendation systems for movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9999baf0",
   "metadata": {},
   "source": [
    "There are basically three types of recommender systems:\n",
    "\n",
    "* **Demographic Filtering**: offer generalized recommendations to every user, based on movie popularity and/or genre. The System recommends the same movies to users with similar demographic features. Since each user is different , this approach is considered to be too simple. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience.\n",
    "\n",
    "* **Content Based Filtering**: suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it.\n",
    "\n",
    "* **Collaborative Filtering**: this system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like its content-based counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7e79d6",
   "metadata": {},
   "source": [
    "# Analyzing different model proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7b2a75",
   "metadata": {},
   "source": [
    "**Demographic Filtering** will create a general recommender that doesn't fit our final product. Although we could add some bias in favour of those movies which are most popular, this is not the type of recommender that we aim to build. A combination of both **Content Based Filtering** and **Collaborative Filtering** (more so the latter), seem to be the correct approach to the recommendation problem. \n",
    "\n",
    "The nature of our data, which consist of mainly users ratings, suggests that the use of the latter is appropiate for the baseline model. In this way, the use of **similarity measures** and **matrix factorization (SVD)** comes to mind immediately.\n",
    "\n",
    "However, matrix factorization has some limitations. If we consider our input $x$ as an initial user query, it will be difficult to use side features and, as a result, the model can only be queried with an user or item present in the training set. There's also the relevance of the recommendations that we just talked about. Popular items tend to be recommended for everyone, especially when using dot product as a similarity measure. It is better to capture **specific user interests**.\n",
    "\n",
    "**Deep neural network (DNN)** models can address these limitations of matrix factorization. DNNs can easily incorporate query features and item features (due to the flexibility of the input layer of the network), which can help capture the specific interests of a user and improve the relevance of recommendations. The input to a DNN can include:\n",
    "\n",
    "* **Dense features**: Watch time (which we don't really have access to), ratings...\n",
    "\n",
    "* **Sparse features**: For example, watch history and country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f5af08",
   "metadata": {},
   "source": [
    "In summary, we have the following:\n",
    "\n",
    "* **Matrix Factorization** is usually the better choice for large corpora. It is easier to scale for an input matrix which is sparse as ours, cheaper to query, and less prone to the \"folding\" phenomena.\n",
    "\n",
    "* **DNN models** can better capture personalized preferences, but are harder to train and more expensive to query. DNN models are preferable to matrix factorization for scoring because DNN models can use more features to better capture relevance. Also, it is usually acceptable for DNN models to \"fold\", since you mostly care about ranking a pre-filtered set of candidates assumed to be relevant.\n",
    "\n",
    "In this notebook, we will explore the DNN alternative for the movies recommendation and, as our dataset is refined, carry out a meaningful implementation of the model of our choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e2981",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7171b7",
   "metadata": {},
   "source": [
    "One possible DNN model is **softmax**, which treats the problem as a multiclass prediction problem in which:\n",
    "\n",
    " * The input is the user query.\n",
    " \n",
    " * The output is a probability vector with size equal to the number of items in the corpus, representing the probability to interact with each item; for example, the probability to watch one movie/director/actor or another.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bff407",
   "metadata": {},
   "source": [
    "The model architecture determines the complexity and expressivity of the model. By adding hidden layers and non-linear activation functions (for example, **ReLU**), the model can capture more complex relationships in the data. However, increasing the number of parameters also typically makes the model harder to train and more expensive to serve. We will denote the output of the last hidden layer by $\\psi(x)\\in\\mathbb{R}^d$.\n",
    "\n",
    "The model maps the output of the last layer, $\\psi(x)$, through a softmax layer to a probability distribution $\\hat{p}=h(\\psi(x)V^T)$ where $h$ is the know softmax function and $V$ is the matrix of weights of the softmax layer. The softmax layer maps a vector of scores $y\\in\\mathbb{R}^n$ (sometimes called the logits) to a probability distribution.\n",
    "\n",
    "Basically, the loss function must compare the following:\n",
    "\n",
    "* $\\hat p$, the output of the softmax layer (a probability distribution).\n",
    "\n",
    "* $p$, the ground truth, representing the items the user has interacted with.\n",
    "\n",
    "For example, you can use the **cross-entropy loss** since you are basically comparing two probability distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d886394",
   "metadata": {},
   "source": [
    "# Softmax training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7db090",
   "metadata": {},
   "source": [
    "The softmax training data consists of the query features, $x$, and a vector of items the user interacted with, $p$ (represented as a probability distribution). The variables of the model are the weights in the different layers (depth is up to us to choose). The model is typically trained using any variant of the **stochastic gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a1917",
   "metadata": {},
   "source": [
    "Computing the gradient of the loss (for a single query $x$) can be prohibitively expensive if the corpus size $n$ is too big. You could set up a system to compute gradients only on the positive items (items that are active in the ground truth vector). However, if the system only trains on positive pairs, the model may suffer from folding.\n",
    "\n",
    "Instead of using all items to compute the gradient (which can be too expensive) or using only positive items (which makes the model prone to folding), you can use **negative sampling**. More precisely, you compute an approximate gradient, using the following items:\n",
    "\n",
    "* All positive items (the ones that appear in the target label).\n",
    "\n",
    "* A sample of negative items ($j\\in\\{1,...,n\\}$). We can do it uniformly or adding weights as we previously suggested to the items with higher scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
